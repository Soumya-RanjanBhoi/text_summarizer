{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "60986213",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\text_Summerizer\\text_summarizer\\tsum_env\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\soumy\\.cache\\huggingface\\hub\\models--facebook--bart-large-cnn. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, pipeline\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"facebook/bart-large-cnn\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/bart-large-cnn\")\n",
    "\n",
    "pipe = pipeline(\"summarization\", model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "3d8cb376",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_text=\"\"\"Once upon a time in a small village nestled between green hills, there lived a young boy named Arjun. He was curious, brave, and loved exploring the forests near his home. One day, while wandering deeper than ever before, he stumbled upon an injured baby deer trapped under a fallen tree. Despite being scared, Arjun carefully lifted the branches and freed the animal.\n",
    "Grateful, the deer followed him back to the village, and soon, the villagers noticed that it had extraordinary abilitiesâ€”it could find hidden water sources, alert people to danger, and even heal minor wounds with its presence. Word of the magical deer spread, attracting travelers and merchants from faraway lands.\n",
    "However, not everyone had good intentions. A greedy landowner wanted to capture the deer and exploit its powers for profit. Arjun, understanding the danger, devised a plan. He led the deer to the hills and created a secret sanctuary where only those with pure hearts could find it.\n",
    "Years passed, and Arjun grew up to become a wise protector of the forest. The deer lived safely, helping those who respected nature. The village prospered, and people learned an important lesson: true courage and kindness can protect what is precious, and sometimes, the smallest acts of bravery can change the fate of many.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "4a20b429",
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_kwargs = {\n",
    "    \"max_length\": 300,\n",
    "    \"min_length\": 100,\n",
    "    \"num_beams\": 4,\n",
    "    \"length_penalty\": 2.0,  # encourages shorter summaries\n",
    "    \"early_stopping\": True,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "5323a727",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 300, but your input_length is only 268. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=134)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'summary_text': 'A young boy named Arjun found an injured baby deer trapped under a fallen tree. The deer had extraordinary abilities, including the ability to find hidden water sources and alert people to danger. A greedy landowner wanted to capture the deer and exploit its powers for profit. Arjun, understanding the danger, devised a plan. He led the deer to the hills and created a secret sanctuary where only those with pure hearts could find it.Years passed, and Arjun grew up to become a wise protector of the forest. The village prospered, and people learned an important lesson.'}]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"summarize: \" + original_text\n",
    "pipe(text, **gen_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "c0d367ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\text_Summerizer\\text_summarizer\\tsum_env\\Lib\\site-packages\\transformers\\modeling_utils.py:4037: UserWarning: Moving the following attributes in the config to the generation config: {'max_length': 142, 'min_length': 56, 'early_stopping': True, 'num_beams': 4, 'length_penalty': 2.0, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('facebook_tokenizer\\\\tokenizer_config.json',\n",
       " 'facebook_tokenizer\\\\special_tokens_map.json',\n",
       " 'facebook_tokenizer\\\\vocab.json',\n",
       " 'facebook_tokenizer\\\\merges.txt',\n",
       " 'facebook_tokenizer\\\\added_tokens.json',\n",
       " 'facebook_tokenizer\\\\tokenizer.json')"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save_pretrained(\"facebbok_model\")\n",
    "tokenizer.save_pretrained(\"facebook_tokenizer\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "12b59517",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\text_Summerizer\\text_summarizer\\tsum_env\\Lib\\site-packages\\transformers\\models\\bart\\configuration_bart.py:177: UserWarning: Please make sure the config includes `forced_bos_token_id=0` in future versions. The config can simply be saved and uploaded again to be fixed.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "imp_model=AutoModelForSeq2SeqLM.from_pretrained(\"D:/text_Summerizer/text_summarizer/research/facebbok_model\")\n",
    "imp_token=AutoTokenizer.from_pretrained(\"D:/text_Summerizer/text_summarizer/research/facebook_tokenizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "720aaf6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "Your max_length is set to 300, but your input_length is only 268. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=134)\n"
     ]
    }
   ],
   "source": [
    "pipe = pipeline(\"summarization\", model=imp_model, tokenizer=imp_token)\n",
    "output=pipe(text, **gen_kwargs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "263399de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A young boy named Arjun found an injured baby deer trapped under a fallen tree. The deer had extraordinary abilities, including the ability to find hidden water sources and alert people to danger. A greedy landowner wanted to capture the deer and exploit its powers for profit. Arjun, understanding the danger, devised a plan. He led the deer to the hills and created a secret sanctuary where only those with pure hearts could find it.Years passed, and Arjun grew up to become a wise protector of the forest. The village prospered, and people learned an important lesson.'"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output[0][\"summary_text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9b3b625",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, pipeline\n",
    "\n",
    "\n",
    "PROMPT_TEMPLATE = (\n",
    "    f\"Translate from english to hinglish:\\n{{en}}\\n---\\nTranslation:\\n\"\n",
    ")\n",
    "model_id = \"nousresearch/llama-2-7b-hf\"\n",
    "peft_model_id = \"nateraw/llama-2-7b-english-to-hinglish\"\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype=torch.bfloat16\n",
    ")\n",
    "model.load_adapter(peft_model_id)\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=model_id,\n",
    ")\n",
    "\n",
    "out = pipe(\n",
    "    PROMPT_TEMPLATE.format(en=output[0][\"summary_text\"]),\n",
    "    return_full_text=False,\n",
    "    do_sample=False,\n",
    "    max_new_tokens=256\n",
    ")[0]['generated_text']\n",
    "print(out)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tsum_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
